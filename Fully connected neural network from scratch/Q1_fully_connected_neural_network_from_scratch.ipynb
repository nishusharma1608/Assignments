{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments for internship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (Fully Connected Neural Network): \n",
    "Consider that you have a neural network with two hidden layers and let X be the input, W1, W2, W3 be the weights of the hidden layers, b1, b2, b3 be the corresponding biases and y be the output. Implement the forward pass and backward pass (you can assume any loss function and activation function). You donâ€™t have to test your code on any data. We will just check the implementation and ask questions based on that (If possible vectorize your code using numpy). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the network \n",
    "'''\n",
    "n_inputs = no. of input neurons\n",
    "n_hidden = a list of size n_hidden_layers which contains no. of neurons in each hidden layer\n",
    "n_outputs = no. of output neurons\n",
    "bias = bias for each neuron in the layer should be specified\n",
    "'''\n",
    "network = list()\n",
    "\n",
    "def create_hidden_layer(n_inputs, n_hidden, bias):\n",
    "    hidden_layer = [{'weights':[random() for j in range(n_inputs)], 'bias' : bias[i]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "\n",
    "def create_output_layer(n_hidden, n_outputs, bias):\n",
    "    output_layer = [{'weights':[random() for j in range(n_hidden)], 'bias' : bias[i]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK : \n",
      "[{'bias': 10, 'weights': [0.13436424411240122, 0.8474337369372327]}, {'bias': 11, 'weights': [0.763774618976614, 0.2550690257394217]}, {'bias': 12, 'weights': [0.49543508709194095, 0.4494910647887381]}]\n",
      "\n",
      "\n",
      "[{'bias': 13, 'weights': [0.651592972722763, 0.7887233511355132, 0.0938595867742349]}, {'bias': 14, 'weights': [0.02834747652200631, 0.8357651039198697, 0.43276706790505337]}, {'bias': 15, 'weights': [0.762280082457942, 0.0021060533511106927, 0.4453871940548014]}, {'bias': 16, 'weights': [0.7215400323407826, 0.22876222127045265, 0.9452706955539223]}]\n",
      "\n",
      "\n",
      "[{'bias': 9, 'weights': [0.9014274576114836, 0.030589983033553536, 0.0254458609934608, 0.5414124727934966]}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "# example\n",
    "seed(1)\n",
    "create_hidden_layer(2,3,[10,11,12])\n",
    "create_hidden_layer(3,4,[13,14,15,16])\n",
    "create_output_layer(4,1,[9])\n",
    "\n",
    "print 'NETWORK : '\n",
    "for layer in network:\n",
    "    print layer\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Forward Propagate\n",
    "We can calculate an output from a neural network by propagating an input signal through each layer until the output layer outputs its values.\n",
    "\n",
    "We call this forward-propagation.\n",
    "\n",
    "It is the technique we will need to generate predictions during training that will need to be corrected, and it is the method we will need after the network is trained to make predictions on new data.\n",
    "\n",
    "We can break forward propagation down into three parts:\n",
    "\n",
    "- Neuron Activation.\n",
    "- Neuron Transfer.\n",
    "- Forward Propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neuron Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is calculated as the weighted sum of inputs and then adding it to the bias unit of that layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate neuron activation \n",
    "def activate(weights, inputs, bias):\n",
    "    activation = bias\n",
    "    for i in range(len(weights)):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neuron Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the traditional transfer function : the sigmoid function for this job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transfer neuron activation\n",
    "import math\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + math.exp(-activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    i = 0\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        print '\\nnew layer'\n",
    "        for neuron in range(len(layer)):\n",
    "            print 'neuron' + str(i)\n",
    "            i+=1\n",
    "            print layer[neuron]['bias']\n",
    "            activation = activate(layer[neuron]['weights'], inputs, layer[neuron]['bias'])\n",
    "            act = transfer(activation)\n",
    "            layer[neuron]['output'] = act\n",
    "            new_inputs.append(act)\n",
    "        inputs = new_inputs\n",
    "        i=0\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new layer\n",
      "neuron0\n",
      "0.763774618977\n",
      "\n",
      "new layer\n",
      "neuron0\n",
      "0.495435087092\n",
      "neuron1\n",
      "0.651592972723\n",
      "[0.6629970129852887, 0.7253160725279748]\n"
     ]
    }
   ],
   "source": [
    "network = [[ {'weights': [0.13436424411240122, 0.8474337369372327],'bias' : 0.763774618976614}],\n",
    "            [{'weights': [0.2550690257394217], 'bias' : 0.49543508709194095}, \n",
    "            {'weights': [0.4494910647887381], 'bias' : 0.651592972722763}]]\n",
    "row = [1, 0, None]\n",
    "output = forward_propagate(network, row)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Back prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error is calculated between the expected outputs and the outputs forward propagated from the network. These errors are then propagated backward through the network from the output layer to the hidden layer, assigning blame for the error and updating weights as they go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transfer Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Error Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != (len(network)-1):\n",
    "            for j in range(len(layer)): # for 1 neuron in present layer\n",
    "                error = 0.0\n",
    "                present_neuron = layer[j]\n",
    "                for prev_neuron in network[i+1]:\n",
    "                    error += prev_neuron['weights'][j] * prev_neuron['delta'] \n",
    "                    error +=  prev_neuron['bias'] * prev_neuron['delta']\n",
    "                    \n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "        \n",
    "#         print network[i]\n",
    "#         print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': 0.7105668883115941, 'bias': 0.763774618976614, 'weights': [0.13436424411240122, 0.8474337369372327], 'delta': -0.005088768681049473}]\n",
      "[{'output': 0.6213859615555266, 'bias': 0.49543508709194095, 'weights': [0.2550690257394217], 'delta': -0.14619064683582808}, {'output': 0.6573693455986976, 'bias': 0.651592972722763, 'weights': [0.4494910647887381], 'delta': 0.0771723774346327}]\n"
     ]
    }
   ],
   "source": [
    "network = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327], 'bias' : 0.763774618976614}],\n",
    "            [{'output': 0.6213859615555266, 'weights': [0.2550690257394217], 'bias' : 0.49543508709194095}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381], 'bias' : 0.651592972722763}]]\n",
    "expected = [0, 1]\n",
    "backward_propagate_error(network, expected)\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Update Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weight = weight + learning_rate * error * input\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
